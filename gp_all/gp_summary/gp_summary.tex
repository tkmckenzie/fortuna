\documentclass{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{cleveref}
\usepackage{fullpage}
\usepackage{graphicx}
\usepackage{subcaption}

\newenvironment{BGVerbatim}
{\VerbatimEnvironment
	\begin{tcolorbox}[
		colback=white
		]%
		\begin{Verbatim}[tabsize=4]}
		{\end{Verbatim}\end{tcolorbox}}

\newcommand{\RR}{\mathbb{R}}
\newcommand{\sample}[1]{{#1}^{[s]}}
\newcommand{\ep}{\varepsilon}

\begin{document}
	
\section{Standard Gaussian Process Fitting}

\label{sec:fitting}

Suppose we have a vector of output variables $y\in\RR^N$, with associated matrix of input variables $X\in\RR^{N\times k}$. Note that $N$ represents the number of observations and $k$ represents the number of input variables. Suppose there is a true relationship between $y$ and $X$ given by $y = f(X)$, where the function $f$ is unknown to the researcher. Gaussian processes aim to approximate $f$ via the following model:

\begin{subequations}
\begin{align}
	\label{eq:GPBase:theta} \theta &\sim g_\theta(\phi_\theta)\\ 
	\label{eq:GPBase:sigma} \sigma &\sim g_\sigma(\phi_\sigma)\\ 
	\label{eq:GPBase:f} f(X) &\sim N(0, K_\theta(X))\\ 
	\label{eq:GPBase:y} y &\sim N(f(X), \sigma^2 I_N). 
\end{align}
\end{subequations}

\noindent \Cref{eq:GPBase:theta} and \cref{eq:GPBase:sigma} represent priors over parameters $\theta$ and $\sigma$. \Cref{eq:GPBase:f} is the prior over values of $f(X)$, formed using kernel function $K_\theta$. A common choice of kernel function is the normal kernel, also referred to as the exponentiated quadratic kernel. This kernel function takes the form

\begin{equation}
	K_\theta(X)_{ij} = \alpha^2 \exp\left(-\frac12 (X_i - X_j)' P^{-1} (X_i - X_j)\right)\mbox{\ \ \ \ }\forall i=1,...,N; j=1,...,N,
\end{equation}

\noindent where $\alpha$ is a positive scalar, $P$ is a positive definite $k\times k$ matrix, and $X_i\in\RR^k$ is the $i$th row of $X$. This implies that $K := K_\theta(X)$ is an $N\times N$ positive definite matrix. Finally, \cref{eq:GPBase:y} relates observations of output, $y$, to their predicted values, $f(X)$.

The above form presents difficulties in sampling for a couple of reasons. First, $f$ has a strong prior dependence on kernel parameters $\theta$ in \cref{eq:GPBase:f} that can lead to inefficient sampling (Betancourt and Girolami 2013). To reduce this prior dependence, first denote the Cholesky decomposition of $K$ as $K = LL'$. Then, we can rewrite the model as 

\begin{subequations}
\begin{align}
	\label{eq:GPChol:theta} \theta &\sim g_\theta(\phi_\theta)\\ 
	\label{eq:GPChol:sigma} \sigma &\sim g_\sigma(\phi_\sigma)\\ 
	\label{eq:GPChol:eta} \eta &\sim N(0, I_N)\\
	\label{eq:GPChol:K} K_\theta(X) &= LL'\\
	\label{eq:GPChol:f} f(X) &= L\eta\\ 
	\label{eq:GPChol:y} y &\sim N(f(X), \sigma^2 I_N). 
\end{align}
\end{subequations}

\noindent This model is equivalent to the one presented above by properties of multiplying a normally-distributed random variable by a constant matrix, which imply that

\begin{subequations}
\begin{align}
	f(X) = L\eta &\sim N(L\times E[\eta], L\times \mbox{var}(\eta)\times L')\\ 
	&\equiv N(L\times 0, L\times I_N \times L')\\ 
	&\equiv N(0, K).
\end{align}
\end{subequations}

This form still presents difficulties in sampling because its posterior distribution is of high dimensionality; using a normal kernel, there are $N + 3$ parameters to sample. We can note, however, that

\begin{equation}
	f(X) \sim N(0, K_\theta(X))
\end{equation}

\noindent and

\begin{equation}
	y \sim N(f(X), \sigma^2 I_N),
\end{equation}

\noindent imply that

\begin{equation}
	y \sim N(0, K_\theta(X) + \sigma^2 I_N).
\end{equation}

\noindent Altogether, the model becomes

\begin{subequations}
\begin{align}
	\label{eq:GPFinal:theta} \theta &\sim g_\theta(\phi_\theta)\\ 
	\label{eq:GPFinal:sigma} \sigma &\sim g_\sigma(\phi_\sigma)\\ 
	\label{eq:GPFinal:y} y &\sim N(0, K_\theta(X) + \sigma^2 I_N). 
\end{align}
\end{subequations}

This model only requires sampling of $\theta$ and $\sigma$, greatly reducing the dimensionality of the model. Further, the mean and variance of $y|f(X)$ can still be derived, described in more detail in the following section.

\section{Standard Gaussian Process Inference}

\label{sec:inference}

Suppose that we have drawn $S$ samples of $\theta$ and $\sigma$ from their posterior distributions. Denote the $s$th sample of $\theta$ as $\sample{\theta}$ and the $s$th sample of $\sigma$ as $\sample{\sigma}$. Suppose that we have a matrix of input variables $X^*\in\RR^{N^*\times k}$ for which we want to predict output $y^*\in\RR^{N^*}$. For given $\theta$ and $\sigma$, it is known that

\begin{equation}
	y^*|x^*, y, x, \theta, \sigma \sim N(A, B),
\end{equation}

\noindent where

\begin{subequations}
\begin{align}
	A &= K_\theta(X^*, X) \Sigma^{-1} y\\
	B &= K_\theta(X^*) - K_\theta(X^*, X) \Sigma^{-1} K_\theta(X^*, X)',
\end{align}
\end{subequations}

\noindent where 

\begin{equation}
	\Sigma = K_\theta(X) + \sigma^2 I_N.
\end{equation}

\noindent Using a normal kernel, the kernel functions above are defined as

\begin{subequations}
\begin{align}
	K_\theta(X)_{ij} &= \alpha^2 \exp\left(-\frac12 (X_i - X_j)' P^{-1} (X_i - X_j)\right)\mbox{\ \ \ \ }\forall i=1,...,N; j=1,...,N\\
	K_\theta(X^*)_{ij} &= \alpha^2 \exp\left(-\frac12 (X^*_i - X^*_j)' P^{-1} (X^*_i - X^*_j)\right)\mbox{\ \ \ \ }\forall i=1,...,N^*; j=1,...,N^*\\
	K_\theta(X^*, X)_{ij} &= \alpha^2 \exp\left(-\frac12 (X^*_i - X_j)' P^{-1} (X^*_i - X_j)\right)\mbox{\ \ \ \ }\forall i=1,...,N^*; j=1,...,N.
\end{align}
\end{subequations}

\noindent In practice it can be more computationally efficient and numerically stable to use the Cholesky decomposition of $\Sigma$ in calculating $A$ and $B$.

Samples from the posterior predictive distribution of $f(y^*)|x^*, y, x$ can be formed by drawing samples of $\sample{y^*}|x^*, y, x, \sample{\theta}, \sample{\sigma}$ and applying $f$ to each of those samples. For example, a distribution of 95\% quantiles of predicted output is given as

\begin{equation}
	q_{0.95}^N(\sample{y^*})|x^*, y, x, \sample{\theta}, \sample{\sigma}.
\end{equation}

Distributional statistics (mean, quantiles, etc.) can then be calculated from those posterior predictive distributions as usual.

\section{Examples}

\subsection{Homoskedastic Gaussian Process Over $\RR$}

Suppose that $y_i = \sin(x_i) / x_i + \ep_i$ for $x_i\in(0, 10)$ and $\ep\sim N(0, 0.05)$. The data used in this analysis is shown in \cref{fig:gphomo:data}.

\begin{figure}[!h]
	\centering
	\begin{subfigure}[b]{0.5\textwidth}
		\includegraphics[width=\textwidth]{\detokenize{R/gp_homosked/plots/data.pdf}}
		\caption{$y_i = \sin(x_i) / x_i + \ep_i$; $\ep_i \sim N(0, 0.05)$}
		\label{fig:gphomo:data}
	\end{subfigure}%
	\begin{subfigure}[b]{0.5\textwidth}
		\includegraphics[width=\textwidth]{\detokenize{R/gp_homosked/plots/fit.pdf}}
		\caption{Gaussian Process Fit and 90\% Confidence Interval}
		\label{fig:gphomo:fit}
	\end{subfigure}
	\caption{Homoskedastic Gaussian Process Over $\RR$}
\end{figure}

The model presented in \cref{sec:fitting} was used to estimate a Gaussian process and methods described \cref{sec:inference} were used to draw inference on the mean and variance of predicted values. Plots of the prediction mean and associated 90\% confidence interval are shown in \cref{fig:gphomo:fit}.

\subsection{Heteroskedastic Gaussian Process Over $\RR$}

Suppose that $y_i = \sin(x_i) / x_i + \ep_i$ for $x_i\in(0, 10)$ and $\ep_i\sim N\left(0, \left(\frac{7}{30}\right)\times\phi\left(\frac{2 (x_i - 5)}{3}\right)\right)$, where $\phi$ is the standard normal density. The data used in this analysis is shown in \cref{fig:gphetero:data}.

\begin{figure}[!h]
	\centering
	\begin{subfigure}[b]{0.5\textwidth}
		\includegraphics[width=\textwidth]{\detokenize{R/gp_hetsked/plots/data.pdf}}
		\caption{$y_i = \sin(x_i) / x_i + \ep_i$; $\ep_i \sim N\left(0, \left(\frac{7}{30}\right)\times\phi\left(\frac{2 (x_i - 5)}{3}\right)\right)$}
		\label{fig:gphetero:data}
	\end{subfigure}%
	\begin{subfigure}[b]{0.5\textwidth}
		\includegraphics[width=\textwidth]{\detokenize{R/gp_hetsked/plots/fit.pdf}}
		\caption{Gaussian Process Fit and 90\% Confidence Interval}
		\label{fig:gphetero:fit}
	\end{subfigure}
	\caption{Heteroskedastic Gaussian Process Over $\RR$}
\end{figure}

This analysis uses a Gaussian process to model both the conditional mean and variance of $y$ about its conditional mean. Specifically, the model is

\begin{subequations}
	\begin{align}
	\label{eq:GPHetsked:theta} \theta &\sim g_\theta(\phi_\theta)\\
	\label{eq:GPHetsked:thetasigma} \theta_\sigma &\sim g_\sigma(\phi_\sigma)\\
	\label{eq:GPHetsked:xi} \xi &\sim g_\xi(\phi_\xi)\\
	\label{eq:GPHetsked:fsigma} f_\sigma(X) &\sim N(0, K_{\theta_\sigma}(X))\\ 
	\label{eq:GPHetsked:f} f(X) &\sim N(0, K_\theta(X))\\
	\label{eq:GPHetsked:sigma} \ln\sigma &\sim N(f_\sigma(X), \xi^2 I_N)\\
	\label{eq:GPHetsked:y} y &\sim N(f(X), \mbox{diag}(\sigma^2)).
	\end{align}
\end{subequations}

\noindent In \cref{eq:GPHetsked:y}, the $\mbox{diag}$ function creates a diagonal matrix from the vector $\sigma^2$, where the $i$th diagonal element of the resulting matrix is the $i$th element of $\sigma^2$. Plots of the prediction mean and associated 90\% confidence interval are shown in \cref{fig:gphetero:fit}. Further, estimated and true standard deviation are shown in \cref{fig:gphetero:variance}. It can be seen that heteroskedasticity in the data is accurately captured by the estimates.

\begin{figure}[!h]
	\centering
	\includegraphics[width=0.5\textwidth]{\detokenize{R/gp_hetsked/plots/variance.pdf}}
	\caption{Estimated and True Variance}
	\label{fig:gphetero:variance}
\end{figure}

%\subsection{Gaussian Process Deterministic Frontier}



\end{document}